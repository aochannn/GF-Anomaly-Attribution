{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb68473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据导入\n",
    "import pandas as pd\n",
    "setattr(pd, \"Int64Index\", pd.Index)\n",
    "setattr(pd, \"Float64Index\", pd.Index)\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from arch import arch_model\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox, het_arch\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "data = pd.read_excel('9*5.xlsx')\n",
    "names=data['投资者分类'].unique()\n",
    "grouped = data.groupby('投资者分类')\n",
    "behavior_values = {}\n",
    "for name, group in grouped:\n",
    "    behavior = group['新行为类型']\n",
    "    behavior_values[name] = behavior\n",
    "#data = pd.read_csv('情绪.csv')\n",
    "#names=data['behavior'].unique()\n",
    "#grouped = data.groupby('behavior')\n",
    "emotion_values = {}\n",
    "for name, group in grouped:\n",
    "    emotion = group['emotion_value']\n",
    "    emotion_values[name] = emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a5016",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#协方差\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "\n",
    "# Function to compute the empirical CDF\n",
    "def empirical_cdf(data):\n",
    "    \"\"\"Compute the empirical CDF for a data series.\"\"\"\n",
    "    return pd.Series(data).rank(method='average') / (len(data) + 1)\n",
    "\n",
    "# Ensure the data length is consistent\n",
    "n = len(next(iter(emotion_values.values())))\n",
    "u_matrix = np.zeros((n, len(emotion_values)))\n",
    "v_matrix = np.zeros((n, len(behavior_values)))\n",
    "\n",
    "# Compute the empirical CDFs and convert to standard normal marginals\n",
    "for i, name in enumerate(names):\n",
    "    if not emotion_values[name].index.equals(behavior_values[name].index):\n",
    "        raise ValueError(f\"Data for {name} is not aligned by date.\")\n",
    "    u = empirical_cdf(emotion_values[name])\n",
    "    v = empirical_cdf(behavior_values[name])\n",
    "    u_matrix[:, i] = norm.ppf(u)\n",
    "    v_matrix[:, i] = norm.ppf(v)\n",
    "\n",
    "# Combine u_matrix and v_matrix into a single matrix for copula fitting\n",
    "copula_data = np.hstack((u_matrix, v_matrix))\n",
    "# Fit a Gaussian Copula\n",
    "cov_estimator = EmpiricalCovariance().fit(copula_data)\n",
    "\n",
    "copula_corr_matrix = cov_estimator.covariance_\n",
    "# Extract the correlation matrix for the emotion and behavior parts separately\n",
    "emotion_behavior_corr_matrix = copula_corr_matrix[:len(names), len(names):]\n",
    "\n",
    "# Convert the results to a DataFrame for easier viewing\n",
    "gaussian_copula_corr_df = pd.DataFrame(emotion_behavior_corr_matrix, index=names, columns=names)\n",
    "gaussian_copula_corr_df.to_excel('新关联系数.xlsx')\n",
    "# Display the correlation DataFrame\n",
    "print(gaussian_copula_corr_df)\n",
    "\n",
    "# Create a figure and axes\n",
    "# fig, axes = plt.subplots(9, 9, figsize=(20, 20))\n",
    "# fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "\n",
    "# Plot each scatter plot\n",
    "# for i, emotion_key in enumerate(names):\n",
    "#     for j, behavior_key in enumerate(names):\n",
    "#         ax = axes[i, j]\n",
    "#         ax.scatter(emotion_values[emotion_key], behavior_values[behavior_key], alpha=0.5)\n",
    "#         ax.set_title(f'{emotion_key} vs {behavior_key}')\n",
    "#         ax.set_xlabel('Emotion Values')\n",
    "#         ax.set_ylabel('Behavior Values')\n",
    "\n",
    "# plt.savefig('scatter_plots.png', dpi=300)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217fd63c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Spearman\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute the ranks for Spearman correlation\n",
    "def rank_data(data):\n",
    "    \"\"\"Compute the ranks for a data series.\"\"\"\n",
    "    return pd.Series(data).rank(method='average')\n",
    "\n",
    "# Ensure the data length is consistent\n",
    "n = len(next(iter(emotion_values.values())))\n",
    "u_matrix = np.zeros((n, len(emotion_values)))\n",
    "v_matrix = np.zeros((n, len(behavior_values)))\n",
    "\n",
    "# Compute the ranks for Spearman correlation\n",
    "for i, name in enumerate(names):\n",
    "    u_matrix[:, i] = rank_data(emotion_values[name])\n",
    "    v_matrix[:, i] = rank_data(behavior_values[name])\n",
    "\n",
    "# Combine u_matrix and v_matrix into a single matrix for correlation computation\n",
    "copula_data = np.hstack((u_matrix, v_matrix))\n",
    "\n",
    "# Compute the Spearman correlation matrix\n",
    "spearman_corr_matrix = np.corrcoef(copula_data, rowvar=False)\n",
    "\n",
    "# Extract the correlation matrix for the emotion and behavior parts separately\n",
    "emotion_behavior_corr_matrix = spearman_corr_matrix[:len(names), len(names):]\n",
    "\n",
    "# Convert the results to a DataFrame for easier viewing\n",
    "spearman_corr_df = pd.DataFrame(emotion_behavior_corr_matrix, index=names, columns=names)\n",
    "spearman_corr_df.to_excel('新关联系数_spearman.xlsx')\n",
    "\n",
    "# Display the correlation DataFrame\n",
    "print(spearman_corr_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5736d9d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pearson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the data length is consistent\n",
    "n = len(next(iter(emotion_values.values())))\n",
    "u_matrix = np.zeros((n, len(emotion_values)))\n",
    "v_matrix = np.zeros((n, len(behavior_values)))\n",
    "\n",
    "# Convert dictionaries to matrices\n",
    "for i, name in enumerate(names):\n",
    "    u_matrix[:, i] = emotion_values[name]\n",
    "    v_matrix[:, i] = behavior_values[name]\n",
    "\n",
    "# Combine u_matrix and v_matrix into a single matrix for correlation computation\n",
    "combined_data = np.hstack((u_matrix, v_matrix))\n",
    "\n",
    "# Compute the Pearson correlation matrix\n",
    "pearson_corr_matrix = np.corrcoef(combined_data, rowvar=False)\n",
    "\n",
    "# Extract the correlation matrix for the emotion and behavior parts separately\n",
    "emotion_behavior_corr_matrix = pearson_corr_matrix[:len(names), len(names):]\n",
    "\n",
    "# Convert the results to a DataFrame for easier viewing\n",
    "pearson_corr_df = pd.DataFrame(emotion_behavior_corr_matrix, index=names, columns=names)\n",
    "pearson_corr_df.to_excel('新关联系数_pearson.xlsx')\n",
    "\n",
    "# Display the correlation DataFrame\n",
    "print(pearson_corr_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#造9*5数据（模拟数据）\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取上传的文件\n",
    "file_path = '新行为.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# 假设投资者分类在 '投资者分类' 列，每个交易日的数据在 '异动时间' 列\n",
    "investor_column = '投资者分类'\n",
    "date_column = '异动时间'\n",
    "behavior_column = '新行为类型'\n",
    "\n",
    "# 初始化新行为类型\n",
    "behavior_categories = ['a', 'b', 'c', 'd', 'e']\n",
    "np.random.seed(0)  # For reproducibility\n",
    "\n",
    "# 获取唯一的投资者分类和交易日期\n",
    "unique_investors = data[investor_column].unique()\n",
    "unique_dates = sorted(data[date_column].unique())\n",
    "\n",
    "# 初始化新行为类型列\n",
    "new_behavior_types = []\n",
    "\n",
    "# 为第一个交易日生成随机行为类型\n",
    "initial_behavior_types = {investor: np.random.choice(behavior_categories) for investor in unique_investors}\n",
    "\n",
    "# 为每个投资者初始化当前行为类型\n",
    "current_behavior_types = initial_behavior_types.copy()\n",
    "\n",
    "# 按日期顺序为每个投资者生成新行为类型\n",
    "for date in unique_dates:\n",
    "    for investor in unique_investors:\n",
    "        # 决定是否改变行为类型\n",
    "        if np.random.rand() < 0.05:\n",
    "            # 改变为其他四种行为类型中的一种\n",
    "            new_behavior = np.random.choice([b for b in behavior_categories if b != current_behavior_types[investor]])\n",
    "            current_behavior_types[investor] = new_behavior\n",
    "        # 记录当前行为类型\n",
    "        new_behavior_types.append(current_behavior_types[investor])\n",
    "\n",
    "# 将新行为类型加入数据中\n",
    "data[behavior_column] = new_behavior_types\n",
    "\n",
    "# 保存结果到新文件\n",
    "output_file_path = '9*5.xlsx'\n",
    "data.to_excel(output_file_path, index=False)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7905ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# 初始化结果矩阵\n",
    "result_matrix = np.zeros((len(names), len(behavior_categories)))\n",
    "\n",
    "# 对行为数据进行标签编码\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# 模型训练并计算关系矩阵\n",
    "for i, investor in enumerate(names):\n",
    "    X = emotion_values[investor]  \n",
    "    y = behavior_values[investor]  \n",
    "    \n",
    "    # 标签编码\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # 将X转换为numpy数组并reshape\n",
    "    X_reshaped = np.array(X).reshape(-1, 1)\n",
    "    \n",
    "    # 拆分训练集和测试集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_encoded, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 逻辑回归模型训练\n",
    "    model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 计算分类准确度\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Investor {investor} - Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    # 存储模型系数（关系矩阵）\n",
    "    for j, category in enumerate(behavior_categories):\n",
    "        coef_index = list(label_encoder.classes_).index(category)\n",
    "        result_matrix[i, j] = model.coef_[coef_index]\n",
    "\n",
    "# 转换结果矩阵为DataFrame\n",
    "result_df = pd.DataFrame(result_matrix, index=names, columns=behavior_categories)\n",
    "\n",
    "# 输出结果\n",
    "result_df.to_excel('逻辑回归关联.xlsx')\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2e640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# 读取上传的文件\n",
    "file_path = '新行为.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# 定义投资者类别和行为类型\n",
    "investor_types = data['投资者分类'].unique()\n",
    "behavior_categories = ['长期类', '短线类', '趋势类', '追涨类', '抄底类']\n",
    "\n",
    "# 定义行为变化概率\n",
    "change_prob = 0.1  # 10%的概率改变行为类型\n",
    "\n",
    "# 生成随机初始权重\n",
    "def generate_random_weights(size):\n",
    "    weights = np.random.rand(size)\n",
    "    return weights / weights.sum()\n",
    "\n",
    "# 生成初始行为类型\n",
    "def generate_initial_behavior(size, weights):\n",
    "    return np.random.choice(behavior_categories, size=size, p=weights)\n",
    "\n",
    "# 更新行为类型\n",
    "def update_behavior(current_behaviors, weights):\n",
    "    change_mask = np.random.rand(len(current_behaviors)) < change_prob\n",
    "    new_behaviors = current_behaviors.copy()\n",
    "    for i, change in enumerate(change_mask):\n",
    "        if change:\n",
    "            new_behaviors[i] = np.random.choice(behavior_categories, p=weights)\n",
    "    return new_behaviors\n",
    "\n",
    "# 模拟每天的行为数据\n",
    "def simulate_daily_behavior(num_investors, initial_weights, num_days):\n",
    "    current_behaviors = generate_initial_behavior(num_investors, initial_weights)\n",
    "    daily_behavior = np.zeros((num_days, len(behavior_categories)), dtype=int)\n",
    "    weights = initial_weights\n",
    "    for day in range(num_days):\n",
    "        weights = generate_random_weights(len(behavior_categories))  # 每天重新生成随机权重\n",
    "        current_behaviors = update_behavior(current_behaviors, weights)\n",
    "        unique, counts = np.unique(current_behaviors, return_counts=True)\n",
    "        behavior_count = dict(zip(unique, counts))\n",
    "        for behavior in behavior_categories:\n",
    "            daily_behavior[day, behavior_categories.index(behavior)] = behavior_count.get(behavior, 0)\n",
    "    return daily_behavior\n",
    "\n",
    "# 初始化数据结构\n",
    "dates = data['异动时间'].unique()\n",
    "num_days = len(dates)\n",
    "\n",
    "# 并行处理每种投资者的数据\n",
    "def process_investor(investor_type, dates, num_days):\n",
    "    if investor_type == '散户':\n",
    "        base_num_investors = np.random.randint(100000, 1000000)  # 模拟中小散户数量\n",
    "    elif investor_type == '大户':\n",
    "        base_num_investors = np.random.randint(1000, 10000)  # 模拟大户数量\n",
    "    elif investor_type == '超有钱户':\n",
    "        base_num_investors = np.random.randint(100, 1000)  # 模拟超大户数量\n",
    "    else:\n",
    "        base_num_investors = np.random.randint(100, 1000)  # 模拟机构投资者数量\n",
    "    \n",
    "    # 随机生成初始权重\n",
    "    initial_weights = generate_random_weights(len(behavior_categories))\n",
    "    \n",
    "    # 模拟每天的行为数据\n",
    "    daily_behavior = simulate_daily_behavior(base_num_investors, initial_weights, num_days)\n",
    "    \n",
    "    rows = []\n",
    "    for day, date in enumerate(dates):\n",
    "        row = {\n",
    "            '异动时间': date,\n",
    "            '投资者分类': investor_type,\n",
    "        }\n",
    "        row.update({behavior_categories[i]: daily_behavior[day, i] for i in range(len(behavior_categories))})\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "# 使用并行处理来优化性能\n",
    "results = Parallel(n_jobs=-1)(delayed(process_investor)(investor, dates, num_days) for investor in investor_types)\n",
    "new_data = [row for result in results for row in result]\n",
    "\n",
    "# 转换为DataFrame\n",
    "new_behavior_df = pd.DataFrame(new_data)\n",
    "\n",
    "# 将新数据合并到原始数据中\n",
    "merged_data = pd.merge(data, new_behavior_df, on=['异动时间', '投资者分类'], how='left')\n",
    "\n",
    "# 保存结果到Excel文件\n",
    "output_file_path = '新行为_带新行为类型_改进版.xlsx'\n",
    "merged_data.to_excel(output_file_path, index=False)\n",
    "\n",
    "merged_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca840589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "\n",
    "# 读取生成的行为数据文件\n",
    "file_path = '新行为_带新行为类型_改进版.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# 定义投资者类别和行为类型\n",
    "investor_types = data['投资者分类'].unique()\n",
    "behavior_categories = ['长期价值类', '短线交易类', '趋势类', '偏向追涨类', '偏向抄底类']\n",
    "\n",
    "# 初始化 emotion_values 和 behavior_values\n",
    "emotion_values = {}\n",
    "behavior_values = {}\n",
    "\n",
    "# 遍历每种投资者\n",
    "for investor in investor_types:\n",
    "    investor_data = data[data['投资者分类'] == investor]\n",
    "    emotion_values[investor] = investor_data['emotion_value']  # 假设情绪值在 '情绪值' 列中\n",
    "    \n",
    "    behavior_values[investor] = {}\n",
    "    for behavior in behavior_categories:\n",
    "        behavior_values[investor][behavior] = investor_data[behavior]\n",
    "        \n",
    "def empirical_cdf(data):\n",
    "    \"\"\"Compute the empirical CDF for a data series.\"\"\"\n",
    "    return pd.Series(data).rank(method='average') / (len(data) + 1)\n",
    "        \n",
    "# 确保数据长度一致\n",
    "def compute_correlation(emotion_data, behavior_data):\n",
    "    \"\"\"Compute the correlation matrix between emotion data and behavior data.\"\"\"\n",
    "    n = len(emotion_data)\n",
    "    m = len(behavior_data)\n",
    "    \n",
    "    u_matrix = np.zeros((n, 1))\n",
    "    v_matrix = np.zeros((n, m))\n",
    "    \n",
    "    # Compute the empirical CDFs and convert to standard normal marginals\n",
    "    u = empirical_cdf(emotion_data)\n",
    "    u_matrix[:, 0] = norm.ppf(u)\n",
    "    \n",
    "    for i, behavior in enumerate(behavior_categories):\n",
    "        v = empirical_cdf(behavior_data[behavior])\n",
    "        v_matrix[:, i] = norm.ppf(v)\n",
    "    \n",
    "    # Combine u_matrix and v_matrix into a single matrix for copula fitting\n",
    "    copula_data = np.hstack((u_matrix, v_matrix))\n",
    "    # Fit a Gaussian Copula\n",
    "    cov_estimator = EmpiricalCovariance().fit(copula_data)\n",
    "    copula_corr_matrix = cov_estimator.covariance_\n",
    "    # Extract the correlation matrix for the emotion and behavior parts separately\n",
    "    emotion_behavior_corr_matrix = copula_corr_matrix[0, 1:]\n",
    "    return emotion_behavior_corr_matrix\n",
    "\n",
    "# 初始化结果矩阵\n",
    "result_matrix = np.zeros((len(investor_types), len(behavior_categories)))\n",
    "\n",
    "# 计算每种投资者的情绪和行为之间的关系\n",
    "for i, investor in enumerate(investor_types):\n",
    "    emotion_data = emotion_values[investor]\n",
    "    behavior_data = behavior_values[investor]\n",
    "    result_matrix[i, :] = compute_correlation(emotion_data, behavior_data)\n",
    "\n",
    "# 转换结果矩阵为 DataFrame\n",
    "result_df = pd.DataFrame(result_matrix, index=investor_types, columns=behavior_categories)\n",
    "\n",
    "# 输出结果\n",
    "output_file_path = '投资者情绪与行为关系矩阵.xlsx'\n",
    "result_df.to_excel(output_file_path)\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5166aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "abc=np.array([True,1,2])+np.array([3,4,False])\n",
    "print(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6665b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b84b72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#草稿页\n",
    "for name in names:\n",
    "    print(name)\n",
    "# Fit ARMA model\n",
    "    p, q = 1,1\n",
    "    arma_model = ARIMA(emotion_values[name], order=(p, 0, q)).fit()\n",
    "    residuals = arma_model.resid\n",
    "\n",
    "    # Fit GARCH model\n",
    "    garch_model = arch_model(residuals, vol='GARCH', p=1, q=1).fit()\n",
    "    conditional_volatility = garch_model.conditional_volatility\n",
    "    standardized_residuals = residuals / conditional_volatility\n",
    "\n",
    "    # Check for no autocorrelation and no conditional heteroscedasticity\n",
    "    lb_test = acorr_ljungbox(standardized_residuals, lags=10, return_df=True)\n",
    "    print(\"Ljung-Box test results for autocorrelation:\\n\", lb_test)\n",
    "\n",
    "    arch_test = het_arch(standardized_residuals, nlags=10)\n",
    "    print(\"ARCH test results for conditional heteroscedasticity:\\n\", arch_test)\n",
    "    arch_test = het_arch(emotion_values[name])\n",
    "    print('ARCH LM Test:')\n",
    "    print('Statistic:', arch_test[0])\n",
    "    print('P-value:', arch_test[1])\n",
    "    print('F-statistic:', arch_test[2])\n",
    "    print('F-test p-value:', arch_test[3])\n",
    "\n",
    "    conditional_variance = conditional_volatility**2\n",
    "\n",
    "    #Plot ACF and PACF of conditional variances\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plot_acf(conditional_variance, lags=40, ax=plt.gca())\n",
    "    plt.title('ACF of Conditional Variances')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plot_pacf(conditional_variance, lags=40, ax=plt.gca())\n",
    "    plt.title('PACF of Conditional Variances')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(standardized_residuals.max())\n",
    "    print(standardized_residuals.min())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34cfea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#看一下acf和pacf来看怎么修改模型\n",
    "#这是原始数据的自相关性图和偏自相关性图\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "for name in names:\n",
    "    print(name)\n",
    "    # Plot ACF and PACF for raw emotion values\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # ACF plot\n",
    "    plot_acf(emotion_values[name], ax=axes[0])\n",
    "    axes[0].set_title('ACF')\n",
    "\n",
    "    # PACF plot\n",
    "    plot_pacf(emotion_values[name], ax=axes[1])\n",
    "    axes[1].set_title('PACF')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb56ab44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#这是ADF平稳性测试 结果P值<0.05 甚至是0 说明非常平稳 不需要做差分计算\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "for name in names:\n",
    "    # Perform ADF test to check stationarity\n",
    "    result = adfuller(emotion_values[name])\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca268a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    print(name)\n",
    "    arch_test = het_arch(emotion_values[name])\n",
    "\n",
    "    # Extracting the results\n",
    "    lm_statistic = arch_test[0]\n",
    "    lm_pvalue = arch_test[1]\n",
    "    f_statistic = arch_test[2]\n",
    "    f_pvalue = arch_test[3]\n",
    "\n",
    "    print('LM Statistic:', lm_statistic)\n",
    "    print('LM p-value:', lm_pvalue)\n",
    "    print('F-Statistic:', f_statistic)\n",
    "    print('F p-value:', f_pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b452a63",
   "metadata": {},
   "source": [
    "根据上述acf和pacf 以及ADF的p值得知 我们的数据具有稳定且非自相关性 ARMA模型不会有太大的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f720a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#对原始数据的分析\n",
    "# Perform seasonal decomposition\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "for name in names:\n",
    "    print(name)\n",
    "    decomposition = seasonal_decompose(emotion_values[name], model='additive', period=12)\n",
    "    # Plot the decomposition\n",
    "    fig = decomposition.plot()\n",
    "    fig.set_size_inches(15, 8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb41dbbb",
   "metadata": {},
   "source": [
    "没有明显trend 没有明显重复规律 残差也比较随机均匀分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e92da5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_garch_model(p, q):\n",
    "    model = arch_model(residuals, vol='Garch', p=p, q=q)\n",
    "    garch_result = model.fit(disp='off')\n",
    "    \n",
    "    # Print the summary\n",
    "    print(f'GARCH({p},{q}) Model Summary:')\n",
    "    print(garch_result.summary())\n",
    "    \n",
    "    # Plot the fitted volatility\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(garch_result.conditional_volatility)\n",
    "    plt.title(f'Fitted Volatility from GARCH({p},{q}) Model')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the residuals\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(garch_result.resid)\n",
    "    plt.title(f'Residuals from GARCH({p},{q}) Model')\n",
    "    plt.show()\n",
    "    \n",
    "    # Ljung-Box test on the residuals\n",
    "    lb_test = acorr_ljungbox(garch_result.resid, lags=[10], return_df=True)\n",
    "    print(f'Ljung-Box Test for GARCH({p},{q}) Residuals:\\n', lb_test)\n",
    "    \n",
    "    # ARCH LM test on the residuals\n",
    "    arch_test = garch_result.arch_lm_test(lags=10)\n",
    "    print(f'ARCH LM Test for GARCH({p},{q}) Residuals:\\n', arch_test)\n",
    "    \n",
    "    arch_lm_results = het_arch(garch_result.resid, nlags=10)\n",
    "    print(arch_lm_results)\n",
    "\n",
    "fit_garch_model(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#看一下arma garch模型后的残差图\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox, het_arch\n",
    "\n",
    "# Plot the residuals to see if they resemble white noise\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(garch_result.resid, label='Residuals')\n",
    "plt.title('Residuals from ARMA-GARCH Model')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform Ljung-Box test on the residuals\n",
    "ljung_box_results = acorr_ljungbox(garch_result.resid, lags=[10], return_df=True)\n",
    "\n",
    "# Check for heteroskedasticity using ARCH LM test on the residuals\n",
    "arch_lm_results = het_arch(garch_result.resid, nlags=10)\n",
    "\n",
    "ljung_box_results, arch_lm_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a99ce",
   "metadata": {},
   "source": [
    "残差自相关和数据波动性的自相关都减少了 但是始终存在聚集性的波动性 接下来尝试不同参数 (1,2) (2,1) (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df91ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_garch_model(1,2)\n",
    "fit_garch_model(2,1)\n",
    "fit_garch_model(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0f29a3",
   "metadata": {},
   "source": [
    "结果相似 尝试egarch和tgarch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809bb684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit EGARCH(1,1) model\n",
    "egarch_model = arch_model(residuals, vol='EGARCH', p=1, q=1)\n",
    "egarch_fit = egarch_model.fit()\n",
    "\n",
    "# Print the results\n",
    "print(egarch_fit.summary())\n",
    "\n",
    "# Plot the fitted volatility\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(egarch_fit.conditional_volatility, label='Fitted Volatility')\n",
    "plt.title('Fitted Volatility from EGARCH(1,1) Model')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform Ljung-Box test on the residuals\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "ljung_box_egarch = acorr_ljungbox(egarch_fit.resid, lags=[10], return_df=True)\n",
    "print(\"Ljung-Box Test for EGARCH Residuals:\")\n",
    "print(ljung_box_egarch)\n",
    "\n",
    "# Perform ARCH LM test on the residuals\n",
    "from statsmodels.stats.diagnostic import het_arch\n",
    "\n",
    "arch_lm_egarch = het_arch(egarch_fit.resid)\n",
    "print(\"ARCH LM Test for EGARCH Residuals:\")\n",
    "print(f\"Statistic: {arch_lm_egarch[0]}\")\n",
    "print(f\"P-value: {arch_lm_egarch[1]}\")\n",
    "print(f\"F-statistic: {arch_lm_egarch[2]}\")\n",
    "print(f\"F-test p-value: {arch_lm_egarch[3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d3193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit TGARCH(1,1) model\n",
    "tgarch_model = arch_model(residuals, vol='GARCH', p=1, o=1, q=1)\n",
    "tgarch_fit = tgarch_model.fit()\n",
    "\n",
    "# Print the results\n",
    "print(tgarch_fit.summary())\n",
    "\n",
    "# Plot the fitted volatility\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(tgarch_fit.conditional_volatility, label='Fitted Volatility')\n",
    "plt.title('Fitted Volatility from TGARCH(1,1) Model')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Perform Ljung-Box test on the residuals\n",
    "ljung_box_tgarch = acorr_ljungbox(tgarch_fit.resid, lags=[10], return_df=True)\n",
    "print(\"Ljung-Box Test for TGARCH Residuals:\")\n",
    "print(ljung_box_tgarch)\n",
    "\n",
    "# Perform ARCH LM test on the residuals\n",
    "arch_lm_tgarch = het_arch(tgarch_fit.resid)\n",
    "print(\"ARCH LM Test for TGARCH Residuals:\")\n",
    "print(f\"Statistic: {arch_lm_tgarch[0]}\")\n",
    "print(f\"P-value: {arch_lm_tgarch[1]}\")\n",
    "print(f\"F-statistic: {arch_lm_tgarch[2]}\")\n",
    "print(f\"F-test p-value: {arch_lm_tgarch[3]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9767543",
   "metadata": {},
   "source": [
    "尝试了garch不同参数以及egarch和tgarch都没有什么明显的变化 接下来尝试从arma入手 找到最好的arma参数后再代入garch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5edd358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Define the p and q parameters to take any value between 0 and 3\n",
    "p = q = range(0, 4)\n",
    "# Generate all different combinations of p, q, and q triplets\n",
    "pdq = list(itertools.product(p, q))\n",
    "\n",
    "# Find the best ARMA model parameters based on AIC\n",
    "best_aic = float('inf')\n",
    "best_order = None\n",
    "for param in pdq:\n",
    "    try:\n",
    "        temp_model = ARIMA(emotion_values, order=(param[0], 0, param[1])).fit()\n",
    "        temp_aic = temp_model.aic\n",
    "        if temp_aic < best_aic:\n",
    "            best_aic = temp_aic\n",
    "            best_order = param\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print('Best ARMA order:', best_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b760c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the best ARMA model\n",
    "best_arma_model = ARIMA(emotion_values, order=(best_order[0], 0, best_order[1])).fit()\n",
    "print(best_arma_model.summary())\n",
    "\n",
    "# Extract residuals and check if they resemble white noise\n",
    "arma_resid = best_arma_model.resid\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(211)\n",
    "plot_acf(arma_resid, lags=40, ax=plt.gca())\n",
    "plt.subplot(212)\n",
    "plot_pacf(arma_resid, lags=40, ax=plt.gca())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1550d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = arch_model(arma_resid, vol='Garch', p=1, q=1)\n",
    "garch_result = model.fit(disp='off')\n",
    "\n",
    "# Print the summary\n",
    "print(f'GARCH(1,1) Model Summary:')\n",
    "print(garch_result.summary())\n",
    "\n",
    "# Plot the fitted volatility\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(garch_result.conditional_volatility)\n",
    "plt.title(f'Fitted Volatility from GARCH(1,1) Model')\n",
    "plt.show()\n",
    "\n",
    "# Plot the residuals\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(garch_result.resid)\n",
    "plt.title(f'Residuals from GARCH(1,1) Model')\n",
    "plt.show()\n",
    "\n",
    "# Ljung-Box test on the residuals\n",
    "lb_test = acorr_ljungbox(garch_result.resid, lags=[10], return_df=True)\n",
    "print(f'Ljung-Box Test for GARCH(1,1) Residuals:\\n', lb_test)\n",
    "\n",
    "# ARCH LM test on the residuals\n",
    "arch_test = garch_result.arch_lm_test(lags=10)\n",
    "print(f'ARCH LM Test for GARCH(1,1) Residuals:\\n', arch_test)\n",
    "\n",
    "arch_lm_results = het_arch(garch_result.resid, nlags=10)\n",
    "print(arch_lm_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1b0d0",
   "metadata": {},
   "source": [
    "找到更好的arma参数并且将结果代入到（1,1）的garch模型中 得到了略微更好的结果 但是离理想结果还是很远"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copulas.multivariate import GaussianMultivariate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import norm, rankdata\n",
    "\n",
    "data = pd.read_excel('新行为.xlsx')\n",
    "behavior_data = data[\"行为系数\"]\n",
    "\n",
    "# Combine the datasets\n",
    "data = np.column_stack((standardized_residuals.values, behavior_data.values))\n",
    "\n",
    "# Step 2: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# Step 3: Compute the empirical CDF\n",
    "ranks = np.apply_along_axis(rankdata, 0, data_standardized)\n",
    "uniform_data = ranks / (len(data_standardized) + 1)\n",
    "\n",
    "# Step 4: Apply the inverse CDF of the standard normal distribution\n",
    "gaussian_data = norm.ppf(uniform_data)\n",
    "\n",
    "# Step 5: Fit a Gaussian copula\n",
    "copula = GaussianCopula()\n",
    "copula.fit(gaussian_data)\n",
    "\n",
    "# Step 6: Extract the correlation matrix\n",
    "correlation_matrix = copula.corr\n",
    "\n",
    "print(\"Correlation matrix estimated from Gaussian copula:\")\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eda62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('calculated_distances.xlsx')\n",
    "# Group the data by '投资者分类'\n",
    "grouped = data.groupby('投资者分类')\n",
    "\n",
    "# Create an empty column for the new behavior values\n",
    "data['行为'] = None\n",
    "\n",
    "# Calculate the new behavior value for each investor category\n",
    "for name, group in grouped:\n",
    "    mean_volume = group['成交量'].mean()\n",
    "    std_volume = group['成交量'].std()\n",
    "    data.loc[group.index, '行为'] = (group['成交量'] - mean_volume) / std_volume\n",
    "\n",
    "data.to_excel('新行为.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde6812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_excel('合并后的行为数据.xlsx')\n",
    "\n",
    "# 投资者分类\n",
    "individual_investors = ['超有钱户', '大户', '散户']\n",
    "institutional_investors = ['基金用户', '专户', '券商', '社保基金', '深股通', '私募']\n",
    "\n",
    "# 计算总占比\n",
    "def calculate_total_ratios():\n",
    "    total_ratio = np.random.uniform(0.900000, 1.000000)\n",
    "    individual_total = np.random.uniform(0.200000, 0.500000)\n",
    "    institutional_total = total_ratio - individual_total\n",
    "    return round(individual_total, 6), round(institutional_total, 6)\n",
    "\n",
    "# 根据涨幅计算买入和卖出比例\n",
    "def calculate_proportions(change_rate):\n",
    "    base_ratio = 0.500000\n",
    "    adjustment = max(min(change_rate * 100, 0.400000), -0.400000)\n",
    "    buy_ratio = base_ratio + adjustment\n",
    "    sell_ratio = base_ratio - adjustment\n",
    "    \n",
    "    total_ratio = buy_ratio + sell_ratio\n",
    "    if total_ratio > 1.0000:\n",
    "        excess = total_ratio - 1.0000\n",
    "        buy_ratio -= excess / 2\n",
    "        sell_ratio -= excess / 2\n",
    "    elif total_ratio < 0.9000:\n",
    "        deficit = 0.9000 - total_ratio\n",
    "        buy_ratio += deficit / 2\n",
    "        sell_ratio += deficit / 2\n",
    "        \n",
    "    #buy_ratio = round(max(min(buy_ratio, 0.8000), 0.2000), 6)\n",
    "    #sell_ratio = round(max(min(sell_ratio, 0.8000), 0.2000), 6)\n",
    "    \n",
    "    return buy_ratio, sell_ratio\n",
    "\n",
    "#计算当天每一个投资者的买入和卖出占比\n",
    "def distribution(individual_total, institutional_total, buy_ratio, sell_ratio):\n",
    "    # Randomly split individual_total into three parts\n",
    "    individual_parts = np.random.dirichlet(np.ones(3), size=1)[0] * individual_total\n",
    "    \n",
    "    # Randomly split institutional_total into six parts\n",
    "    institutional_parts = np.random.dirichlet(np.ones(6), size=1)[0] * institutional_total\n",
    "    \n",
    "    # Combine parts into a single list\n",
    "    all_parts = list(individual_parts) + list(institutional_parts)\n",
    "    \n",
    "    # Initialize result dictionary\n",
    "    distribution_dict = {}\n",
    "    \n",
    "    # Calculate buy and sell proportions for each part\n",
    "    for idx, part in enumerate(all_parts):\n",
    "        buy_key = chr(97 + idx) + '_buy'  # 'a_buy', 'b_buy', ...\n",
    "        sell_key = chr(97 + idx) + '_sell'  # 'a_sell', 'b_sell', ...\n",
    "        distribution_dict[buy_key] = round(part * buy_ratio, 6)\n",
    "        distribution_dict[sell_key] = round(part * sell_ratio, 6)\n",
    "    \n",
    "    return distribution_dict\n",
    "\n",
    "# 分配每类投资者的成交量\n",
    "def allocate_volumes(data):\n",
    "    for date in data['异动时间'].unique():\n",
    "        individual_total, institutional_total = calculate_total_ratios()\n",
    "        buy_ratio, sell_ratio = calculate_proportions(data.loc[data['异动时间'] == date, 'change_rate'].values[0])\n",
    "        dist = distribution(individual_total, institutional_total, buy_ratio, sell_ratio)\n",
    "\n",
    "        for i, investor_type in enumerate(individual_investors):\n",
    "            data.loc[(data['异动时间'] == date) & (data['投资者分类'] == investor_type), '主动买入占比'] = dist[chr(97 + i) + '_buy']\n",
    "            data.loc[(data['异动时间'] == date) & (data['投资者分类'] == investor_type), '主动卖出占比'] = dist[chr(97 + i) + '_sell']\n",
    "        \n",
    "        # Assign values to institutional investors\n",
    "        for i, investor_type in enumerate(institutional_investors):\n",
    "            data.loc[(data['异动时间'] == date) & (data['投资者分类'] == investor_type), '主动买入占比'] = dist[chr(100 + i) + '_buy']\n",
    "            data.loc[(data['异动时间'] == date) & (data['投资者分类'] == investor_type), '主动卖出占比'] = dist[chr(100 + i) + '_sell']\n",
    "    \n",
    "    # 分配成交量\n",
    "    for i, row in data.iterrows():\n",
    "        date = row['异动时间']\n",
    "        volume = row['volume']\n",
    "        data.at[i, '成交量'] = round(volume * (row['主动买入占比'] + row['主动卖出占比']))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 分配成交量和买卖占比\n",
    "new_behavior_data = allocate_volumes(data)\n",
    "\n",
    "# 输出新数据\n",
    "new_behavior_data.to_excel('新行为数据.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ed0658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取行为数据\n",
    "behavior_data = pd.read_csv('行为.csv')\n",
    "# 读取市场数据\n",
    "market_data = pd.read_excel('行情.xlsx')\n",
    "\n",
    "# 确保异动时间列的日期格式一致\n",
    "behavior_data['异动时间'] = pd.to_datetime(behavior_data['异动时间'])\n",
    "market_data['date'] = pd.to_datetime(market_data['date'])\n",
    "\n",
    "# 保留市场数据中的交易日\n",
    "filtered_behavior_data = behavior_data[behavior_data['异动时间'].isin(market_data['date'])]\n",
    "\n",
    "# 合并市场数据\n",
    "merged_data = pd.merge(filtered_behavior_data, market_data, left_on='异动时间', right_on='date')\n",
    "\n",
    "# 移除合并后不需要的列（如 'date' 列）\n",
    "merged_data = merged_data.drop(columns=['date'])\n",
    "\n",
    "# 输出合并后的数据\n",
    "merged_data.to_excel('合并后的行为数据.xlsx', index=False)\n",
    "\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502fae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
